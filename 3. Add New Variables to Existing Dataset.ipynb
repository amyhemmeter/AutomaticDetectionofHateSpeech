{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in dataset as well as the tfidf csv we created in the last Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hs_merged.csv', encoding = \"ISO-8859-1\", index_col=0)\n",
    "tf = pd.read_csv('tfidf.csv', encoding = \"ISO-8859-1\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of the sexist terms, racist terms and other terms. Then put those three lists into a list of lists (\"classes\"). Create a \"bad list\" of tfidf terms that we think are not predictive (\"colin\" is someone's name, for instance), and go through each list and remove those terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t = list(tf['Sexist_terms'])\n",
    "r_t = list(tf['Racist_terms']) \n",
    "o_t = list(tf['Other_terms'])\n",
    "classes = [s_t,r_t,o_t]\n",
    "\n",
    "bad_list = ['colin','im','dont','ok','u','af','na','ur','p','^.^','lt','am','|']\n",
    "\n",
    "for i in classes:\n",
    "    for k in i:\n",
    "        if k in bad_list:\n",
    "            i.remove(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of tweets. For each list, and then for each term in that list, create an empty list. For each tweet in the dataset, check to see whether that tweet contains the term. If yes, add a 1 to the list. If not, add a 0 to the list. Add that list to the dataframe under a name that is the same as the term. To check whether the terms were added to your dataframe, uncomment the print statement at the end. You should see all tfidf terms listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df['Tweets']) \n",
    "\n",
    "for l in classes:\n",
    "    for i in l:\n",
    "        t = []\n",
    "        for d in tweets:\n",
    "            tfm = re.search(i, d)\n",
    "            if tfm:\n",
    "                t.append(1)\n",
    "            else:\n",
    "                t.append(0)\n",
    "        df[i] = t    \n",
    "\n",
    "    \n",
    "names = df.columns.values \n",
    "#print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do something similar with hashtags, though we need to do a little preprocessing. Because this variable is not a simple list of hashtags, and also not (confusingly) a dictionary as it appears (to see what I'm talking about, uncomment the print statement above), we have to go through and extract the hashtags from the string for ourselves. Because it presents as a dictionary with keys, pull out all of the words and then get rid of the keywords \"text\" and \"indices\". Also convert to lowercase. Then create a set of only the unique values to use as variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = list(df['Hashtags'])\n",
    "htags = []\n",
    "\n",
    "for hashtag in hashtags:\n",
    "    texts = re.findall(r\"'\\w+'\", hashtag)\n",
    "    for i in texts:\n",
    "        if i != 'text' and i != 'indices':\n",
    "            htags.append(i.lower())\n",
    "\n",
    "htags=set(htags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the actual hashtags, we have to go through the original hashtags variable and for each observation check whether a term in the hashtag list we created above is in the original variable. If it is, add a 1 to the list we create in our for loop. If not, add a zero. (This is very similar to what we did for our tfidf terms in the tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_h = list(df['Hashtags'])\n",
    "\n",
    "for i in htags:\n",
    "    list_i = []\n",
    "    for k in list_h:\n",
    "        ht = re.search(i,k)\n",
    "        if ht:\n",
    "            list_i.append(1)\n",
    "        else:\n",
    "            list_i.append(0)\n",
    "    df[i] = list_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last new variable that we're adding is values for sentiment analysis. Create an analyzer object called \"analyzer\" with Vader's SentimentIntensityAnalyzer() function. Since hate speech is associated with negative valency, we've created two lists to hold the values for the negative sentiment and the compound (overall) sentiment. The for loop goes through all of the tweets in the tweets variable from the dataframe, which we've converted into a list, and assigns sentiment to each tweet. The values for the negative and compound sentiment are then appended to the empty lists we created for each, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = list(df.Tweets)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "negative = []\n",
    "compound = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    vs = analyzer.polarity_scores(tweets[i])\n",
    "    negative.append(vs.get(\"neg\"))\n",
    "    compound.append(vs.get(\"compound\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add \"Neg_Sent\" and \"Compound\" variables to the dataframe, set each to the values from their lists above. To see the resulting dataframe, uncomment the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Neg_Sent'] = negative\n",
    "df['Compound'] = compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sorri</th>\n",
       "      <th>celin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324114200450437120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326286656854454273</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sorri  celin\n",
       "ID                              \n",
       "319399851215433729      0      0\n",
       "320817818222358529      0      0\n",
       "324114200450437120      0      0\n",
       "326286656854454273      0      0\n",
       "381988216292655104      0      0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Save dataset with these new variables as a csv: this is our official final dataset that we will use in a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"full_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
