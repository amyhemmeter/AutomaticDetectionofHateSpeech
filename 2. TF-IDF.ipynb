{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the csvs we created and convert them into lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sexism = pd.read_csv('sexist_tokens.csv', encoding = \"ISO-8859-1\", index_col=0)\n",
    "racism = pd.read_csv('racist_tokens.csv', encoding = \"ISO-8859-1\", index_col=0)\n",
    "other = pd.read_csv('other_tokens.csv', encoding = \"ISO-8859-1\", index_col=0)\n",
    "\n",
    "sexism = list(sexism['Sexist tokens'])\n",
    "racism = list(racism['Racist tokens'])\n",
    "other = list(other['Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert the term vectors into a gensim dictionary, then create the TFIDF vectors from the bag-of-words corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Convert term vectors into gensim dictionary\n",
    "\n",
    "dict = gensim.corpora.Dictionary(term_vec)\n",
    "\n",
    "corp = []\n",
    "for i in range(0, len(term_vec)):\n",
    "    corp.append( dict.doc2bow( term_vec[ i ] ) )\n",
    "\n",
    "#  Create TFIDF vectors based on term vectors bag-of-word corpora\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel( corp )\n",
    "\n",
    "tfidf = [ ]\n",
    "for i in range( 0, len( corp ) ):\n",
    "    tfidf.append( tfidf_model[ corp[ i ] ] )\n",
    "\n",
    "dictionary = dict.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list called \"terms.\" this is where we'll be appending the different TFIDF words for each list. (Terms will be a list of lists.) This for loop prints the words with the top 100 TFIDF values for each list, by sorting them and then taking the first 100 of the list, then matching the indices to the gensim dictionary above. We append each term into an internal list, then append the whole list into the terms list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 terms of note:\n",
      "colin\n",
      "sorri\n",
      "honestli\n",
      "femin\n",
      "ref\n",
      "footbal\n",
      "kitchen\n",
      "cunt\n",
      "horribl\n",
      "dude\n",
      "im\n",
      "driver\n",
      "dont\n",
      "sport\n",
      "sportscent\n",
      "aw\n",
      "chick\n",
      "espn\n",
      "annoy\n",
      "ladi\n",
      "ok\n",
      "comedian\n",
      "coach\n",
      "dish\n",
      "score\n",
      "blond\n",
      "rugbi\n",
      "standard\n",
      "omg\n",
      "analyst\n",
      "swear\n",
      "y'all\n",
      "hoe\n",
      "u\n",
      "af\n",
      "shave\n",
      "hair\n",
      "weird\n",
      "ghost\n",
      "mom\n",
      "na\n",
      "idk\n",
      "rap\n",
      "wow\n",
      "basketbal\n",
      "slut\n",
      "wrestl\n",
      "fem\n",
      "tit\n",
      "wnba\n",
      "ur\n",
      "gender\n",
      "ufc\n",
      "charact\n",
      "sister\n",
      "though\n",
      "crazi\n",
      "fan\n",
      "team\n",
      "gon\n",
      "argu\n",
      "wonder\n",
      "smile\n",
      "andr\n",
      "rapper\n",
      "cant\n",
      "hockey\n",
      "eye\n",
      "leagu\n",
      "tonight\n",
      "miss\n",
      "am\n",
      "tan\n",
      "thor\n",
      "skank\n",
      "ghostbust\n",
      "bimbo\n",
      "parenthood\n",
      "abort\n",
      "gap\n",
      "romney\n",
      "movement\n",
      "adopt\n",
      "ford\n",
      "valenti\n",
      "kati\n",
      "nikki\n",
      "hooker\n",
      "sausag\n",
      "trashi\n",
      "sassi\n",
      "tart\n",
      "deconstruct\n",
      "promo\n",
      "anni\n",
      "kat\n",
      "strateg\n",
      "lloyd\n",
      "celin\n",
      "\n",
      "Doc 2 terms of note:\n",
      "testimoni\n",
      "terrorist\n",
      "islamofascist\n",
      "inhuman\n",
      "religion\n",
      "outlaw\n",
      "prophet\n",
      "pedophil\n",
      "caravan\n",
      "robber\n",
      "slave\n",
      "trader\n",
      "scum\n",
      "israel\n",
      "extermin\n",
      "allah\n",
      "fanat\n",
      "islamophob\n",
      "reform\n",
      "convert\n",
      "jihad\n",
      "moham\n",
      "mosul\n",
      "peac\n",
      "slaveri\n",
      "imperi\n",
      "tyrant\n",
      "pervert\n",
      "sharia\n",
      "phoni\n",
      "religi\n",
      "isra\n",
      "microbrain\n",
      "slaughter\n",
      "conquer\n",
      "crusad\n",
      "saudi\n",
      "europ\n",
      "arabian\n",
      "mankind\n",
      "khybar\n",
      "hindu\n",
      "pakistan\n",
      "buddhist\n",
      "contribut\n",
      "freedom\n",
      "ezidi\n",
      "nazi\n",
      "subject\n",
      "invad\n",
      "politician\n",
      "cornerston\n",
      "expedit\n",
      "behead\n",
      "enslav\n",
      "mainstream\n",
      "filth\n",
      "citizen\n",
      "spain\n",
      "islamist\n",
      "arab\n",
      "divis\n",
      "taliban\n",
      "properti\n",
      "islamolunat\n",
      "palestinian\n",
      "hama\n",
      "peninsula\n",
      "jewish\n",
      "imperialist\n",
      "shia\n",
      "non-muslim\n",
      "elect\n",
      "daesh\n",
      "medina\n",
      "koban\n",
      "discoveri\n",
      "persian\n",
      "thug\n",
      "madrassa\n",
      "egotist\n",
      "tribe\n",
      "third\n",
      "pedophelia\n",
      "daeshbag\n",
      "militia\n",
      "slaver\n",
      "occupi\n",
      "lao\n",
      "islamophobia\n",
      "naziphobia\n",
      "graduat\n",
      "mujahedeen\n",
      "palestin\n",
      "ottoman\n",
      "oppon\n",
      "roof\n",
      "mob\n",
      "bangladesh\n",
      "\n",
      "Doc 3 terms of note:\n",
      "colin\n",
      "sorri\n",
      "eat\n",
      "kitchen\n",
      "instant\n",
      "dude\n",
      "minut\n",
      "ok\n",
      "dish\n",
      "score\n",
      "omg\n",
      "u\n",
      "weird\n",
      "na\n",
      "wow\n",
      "lt\n",
      "though\n",
      "p\n",
      "team\n",
      "round\n",
      "andr\n",
      "judg\n",
      "uh\n",
      "tonight\n",
      "mention\n",
      "miss\n",
      "am\n",
      "contest\n",
      "awesom\n",
      "gamerg\n",
      "gg\n",
      "plate\n",
      "kati\n",
      "nikki\n",
      "harass\n",
      "tech\n",
      "tart\n",
      "dessert\n",
      "deconstruct\n",
      "lemon\n",
      "anni\n",
      "ash\n",
      "camilla\n",
      "sheri\n",
      "kat\n",
      "elimin\n",
      "pete\n",
      "strateg\n",
      "lloyd\n",
      "manu\n",
      "emili\n",
      "sudden\n",
      "terrorist\n",
      "religion\n",
      "moham\n",
      "mosul\n",
      "microbrain\n",
      "daesh\n",
      "koban\n",
      "code\n",
      "shitti\n",
      "yay\n",
      "puppi\n",
      "random\n",
      "bulli\n",
      "coffe\n",
      "email\n",
      "russia\n",
      "user\n",
      "chicken\n",
      "updat\n",
      "ypg/ypj\n",
      "pesh\n",
      "coalit\n",
      "facebook\n",
      "|\n",
      "apart\n",
      "wadhwa\n",
      "tomorrow\n",
      "freebsd\n",
      "github\n",
      "yup\n",
      "ethic\n",
      "gater\n",
      "perl\n",
      "dev\n",
      "leo\n",
      "^.^\n",
      "upset\n",
      "gdc\n",
      "appl\n",
      "edit\n",
      "bacon\n",
      "oapi\n",
      "liver\n",
      "jac\n",
      "shaz\n",
      "yummi\n",
      "mapl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = []\n",
    "\n",
    "for i in range(0,len(tfidf)):\n",
    "    term = []\n",
    "    sorted_values = sorted(tfidf[i], key=lambda x: x[1])\n",
    "    first = (len(sorted_values))-99\n",
    "    top_100 = sorted_values[first:]\n",
    "    values =  [int(i[0]) for i in top_100]\n",
    "    num = i+1\n",
    "    print(\"Doc\",str(num),\"terms of note:\")\n",
    "    for key,val in dictionary.items():\n",
    "        if val in values:\n",
    "            print(key)\n",
    "            term.append(key)\n",
    "    print()\n",
    "    terms.append(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe of tfidf terms and save that dataframe as a csv file to access in the next Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame({'Sexist_terms': terms[0], 'Racist_terms': terms[1],'Other_terms': terms[2]})\n",
    "tfidf.to_csv('tfidf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
