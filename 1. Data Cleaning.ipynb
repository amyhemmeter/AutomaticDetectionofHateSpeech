{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the hatespeech dataset that we created when we queried the Twitter API for the tweets and their associated metadata. Then read in the original dataset with the target variable labels, created by Zeerak Waseem. To get a sense of any of these datasets, uncomment the print statements to print the first 6 rows. Do an inner merge on the two dataframes to match up the target variable, class, from the original dataset with all of the data we've collected in the larger dataframe. We'll call the resulting dataframe \"hs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hs = pd.read_csv('hatespeech.csv', encoding=\"ISO-8859-1\",index_col=6, keep_default_na=False)\n",
    "#print(hs.head())\n",
    "\n",
    "orig = pd.read_csv('NAACL_SRW_2016.csv', index_col=0, header=None)\n",
    "orig.index.name = 'ID'\n",
    "orig = orig.rename(columns={1: 'Class'})\n",
    "orig.index = orig.index.astype(str)\n",
    "#print(orig.head())\n",
    "\n",
    "#merging the two dataframes\n",
    "hs = pd.merge(hs, orig, how='inner', left_index=True, right_index=True)\n",
    "#print(hs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Split the dataframe into three different dataframes, one with only the sexist tweets, one with only the racist tweets, and one with the \"none\" class. Create lists from the Tweets variable of each of the mini-dataframes. We will treat these as our \"documents\" for TFIDF when we run our TFIDF in the next Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sexism =  hs.loc[hs['Class'] == 'sexism']\n",
    "racism = hs.loc[hs['Class'] == 'racism']\n",
    "none = hs.loc[hs['Class'] == 'none']\n",
    "\n",
    "s_tweets = list(sexism.Tweets)\n",
    "r_tweets = list(racism.Tweets)\n",
    "n_tweets = list(none.Tweets)\n",
    "\n",
    "class_list = [s_tweets, r_tweets, n_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation is on a list that comes standard from nltk, but in our case we're going to use our own because @ and # are meaningful on social media and we'll want to access that information in our other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = [':',';','!',',','.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use regular expressions to get rid of some of the redundant variables that we have in the metadata. For example, we get rid of words that have # in front of them, because those are hashtags and we'll deal with them in a different way later. We also get rid of some strange things like the &amp and the RT value, because those have different meanings on social media as well. We have to do do this before tokenizing, because the tokenizer splits words on punctuation. For example, #hashtag would be tokenized as [\"#\", \"hashtag\"]. It is easier to move them first, then get rid of punctuation afterwards, like we've done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_vec = []\n",
    "\n",
    "for i in class_list:\n",
    "    doc = []\n",
    "    doc2 = []\n",
    "    for d in i:\n",
    "        d = re.sub('\\@\\w+', '', d)\n",
    "        d = re.sub('\\#\\w+','', d)\n",
    "        d = re.sub('\\#','',d)\n",
    "        d = re.sub('RT','',d)\n",
    "        d = re.sub('&amp;','',d)\n",
    "        d = re.sub('[0-9]+','',d)\n",
    "        d = re.sub('//t.co/\\w+','',d)\n",
    "        d = re.sub('w//','',d)\n",
    "        d = d.lower()\n",
    "        doc.append( nltk.word_tokenize( d ) )\n",
    "    for j in doc:\n",
    "        for s in j:\n",
    "            if s not in punctuation:\n",
    "                doc2.append(s)\n",
    "    term_vec.append(doc2)\n",
    "\n",
    "#print(term_vec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This removes standard stopwords from the list of terms for each of the classes. To see a sample of the list without the stopwords, uncomment the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words( 'english' )\n",
    "\n",
    "for j in term_vec:\n",
    "    for i in j:\n",
    "        contractions = re.match('\\'', i)\n",
    "        if i in stop_words or contractions:\n",
    "            j.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take these tokens and run them through a Porter stemmer. This reduces words like \"run\" and \"runs\" and reduces them both to \"run\" -- by reducing words to their root we reduce redundancy and get a better sense of their root meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'yeah', 'colin', 'smash', 'girl', 'swear', 'sexist', 'honestli', 'stand', 'woman', 'colleg', 'footbal', 'announc', 'espn', 'call', 'sexist', 'think', 'women', 'serious', 'lack', 'knowledg', 'l', 'come', 'femin', 'call', 'sexist', 'femal', 'realli', 'need', 'stop']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range( 0, len( term_vec ) ):\n",
    "    for j in range( 0, len( term_vec[ i ] ) ):\n",
    "        term_vec[ i ][ j ] = porter.stem( term_vec[ i ][ j ] )\n",
    "\n",
    "print(term_vec[0][0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now relatively clean! For the sake of the length of this notebook, we're going to save each of the lists of words from each class into a separate csv to upload into the next Notebook where we will do our TFIDF analysis. We will also save the bigger dataset as 'hs_merged.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sexist = pd.DataFrame({'Sexist tokens': term_vec[0]})\n",
    "racist = pd.DataFrame({'Racist tokens': term_vec[1]})\n",
    "other = pd.DataFrame({'Other':term_vec[2]})\n",
    "\n",
    "racist.to_csv('racist_tokens.csv')\n",
    "sexist.to_csv('sexist_tokens.csv')\n",
    "other.to_csv('other_tokens.csv')\n",
    "\n",
    "hs.to_csv('hs_merged.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
