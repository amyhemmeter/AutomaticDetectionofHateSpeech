{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a couple of low-level packages(i.e. no scikit learn or pandas or numpy or anything fancy) for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the csv file, add each line to a list and adjust for header (cut it off) if relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadCsv(filename, header=True):\n",
    "    with open(filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        dataset = []\n",
    "        for row in csvReader:\n",
    "            dataset.append(row)\n",
    "    if header: dataset = dataset[1:]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the dataset into two different lists: data (the tweet, in this case) and target variables. Convert the tags 'sexism' and 'racism' into one tag representing hate speech, 1, and 'none' tweets into 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_data(dataset):\n",
    "    data = []\n",
    "    target = []\n",
    "    for i in dataset:\n",
    "        data.append(i[1])\n",
    "        if i[2] == 'sexism':\n",
    "            target.append(1)\n",
    "        elif i[2] == 'racism':\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a class called HateSpeechDetector, which does some simple data cleaning and prep tasks for the strings (gets rid of punctuation in the clean() function, tokenizes the text using regular expressions in the tokenize() function, and gets the word counts in the get_word_counts() function). It then goes into the actual modeling part of the Naive Bayes function. In order to not break up the class, I will add more comments within the code cell rather than in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HateSpeechDetector(object):\n",
    "    #Gets rid of punctuation from the string\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    " \n",
    "    #Converts all text to lowercase and splits the strings into one-word tokens.\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    " \n",
    "    #Get the frequency counts of each word\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "    \n",
    "    #Fit the model to the training data\n",
    "    def fit(self, X, Y):\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    " \n",
    "        ###Calculate the log class priors###\n",
    "        '''When I tried getting just the priors without doing a log I got very bad results; I \n",
    "        referenced the Stanford NLP website's page on Naive Bayes text classification\n",
    "        (https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "        where they explain this is due to floating point underflow - so I instead took the sum \n",
    "        of the logs of the probabilitiies rather than the product of the probabilities, \n",
    "        as they suggested'''\n",
    "        n = len(X)\n",
    "        self.log_class_priors['hatespeech'] = math.log(sum(1 for label in Y if label == 1) / n)\n",
    "        self.log_class_priors['none'] = math.log(sum(1 for label in Y if label == 0) / n)\n",
    "        \n",
    "        #initialize the dictionaries for hatespeech and none\n",
    "        self.word_counts['hatespeech'] = {}\n",
    "        self.word_counts['none'] = {}\n",
    " \n",
    "        #count the words for each category \n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'hatespeech' if y == 1 else 'none'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    " \n",
    "                self.word_counts[c][word] += count\n",
    "    \n",
    "    #predict based on the test set\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            \n",
    "            hatespeech_score = 0\n",
    "            none_score = 0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "            \n",
    "            # use Laplace smoothing to account for sparsity\n",
    "                log_w_given_hatespeech = math.log( (self.word_counts['hatespeech'].get(word, 0.0) + 1) / (sum(self.word_counts['hatespeech'].values()) + len(self.vocab)) )\n",
    "                log_w_given_none = math.log( (self.word_counts['none'].get(word, 0.0) + 1) / (sum(self.word_counts['none'].values()) + len(self.vocab)) )\n",
    " \n",
    "                hatespeech_score += log_w_given_hatespeech\n",
    "                none_score += log_w_given_none\n",
    " \n",
    "            hatespeech_score += self.log_class_priors['hatespeech']\n",
    "            none_score += self.log_class_priors['none']\n",
    " \n",
    "            #assign result based on which score is larger\n",
    "            if hatespeech_score > none_score:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function here brings all of these functions together and gets evaluation metrics (accuracy and F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #load in hatespeech dataset\n",
    "    ds = loadCsv('hs_for_naive.csv')\n",
    "    \n",
    "    random.shuffle(ds)\n",
    "    \n",
    "    #divide hatespeech dataset into the X values (the tweet string) and y values (class)\n",
    "    X, y = divide_data(ds)\n",
    "    #create HateSpeechDetector object\n",
    "    MNB = HateSpeechDetector()\n",
    "    #fit the first 15000 values\n",
    "    MNB.fit(X[15000:], y[15000:])\n",
    "    \n",
    "    #test with the last 100 values of the dataset\n",
    "    pred = MNB.predict(X[:100])\n",
    "    true = y[:100]\n",
    "    \n",
    "    #calculate accuracy, recall, and precision\n",
    "    accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "    recall = sum(1 for i in range(len(pred)) if (pred[i] == 1 & true[i] == 1))/(sum(true))\n",
    "    precision = sum(1 for i in range(len(pred)) if (pred[i] == 1 & true[i] == 1))/(sum(pred))\n",
    "    \n",
    "    #calculate F1\n",
    "    F1 = 2*((recall*precision)/(recall+precision))\n",
    "    print(\"Accuracy: \" + \"{0:.4f}\".format(accuracy))\n",
    "    print(\"F1: \" + \"{0:.4f}\".format(F1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    " main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
