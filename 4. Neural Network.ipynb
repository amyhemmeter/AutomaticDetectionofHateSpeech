{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the final dataset, with all of our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanc_000\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('full_dataset.csv', encoding = \"ISO-8859-1\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this version of the neural net, we decided to keep only the binary variables for tfidf and the hashtag variables, in addition to the negative sentiment and compound sentiment variables as predictors. These are variables 11 through 1576. Create a dataframe out of only these columns below. To get an idea of what this looks like, uncomment the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    sorri  celin  bangladesh  mapl  honestli  femin  ref  \\\n",
      "ID                                                                         \n",
      "319399851215433729      0      0           0     0         0      0    0   \n",
      "320817818222358529      0      0           0     0         0      0    0   \n",
      "324114200450437120      0      0           0     0         0      0    0   \n",
      "326286656854454273      0      0           0     0         0      0    0   \n",
      "381988216292655104      0      0           0     0         0      0    0   \n",
      "\n",
      "                    footbal  kitchen  cunt    ...     'needarethinkinformat'  \\\n",
      "ID                                            ...                              \n",
      "319399851215433729        0        0     0    ...                          0   \n",
      "320817818222358529        0        0     0    ...                          0   \n",
      "324114200450437120        0        0     0    ...                          0   \n",
      "326286656854454273        0        0     0    ...                          0   \n",
      "381988216292655104        1        0     0    ...                          0   \n",
      "\n",
      "                    'blondemoment'  'fatalattraction'  'copenhagen'  \\\n",
      "ID                                                                    \n",
      "319399851215433729               0                  0             0   \n",
      "320817818222358529               0                  0             0   \n",
      "324114200450437120               0                  0             0   \n",
      "326286656854454273               0                  0             0   \n",
      "381988216292655104               0                  0             0   \n",
      "\n",
      "                    'malegaze'  'tbh'  'tikrit'  'foodie'  Neg_Sent  Compound  \n",
      "ID                                                                             \n",
      "319399851215433729           0      0         0         0     0.000    0.7793  \n",
      "320817818222358529           0      0         0         0     0.184   -0.4019  \n",
      "324114200450437120           0      0         0         0     0.208   -0.4678  \n",
      "326286656854454273           0      0         0         0     0.000    0.0000  \n",
      "381988216292655104           0      0         0         0     0.213   -0.5139  \n",
      "\n",
      "[5 rows x 1565 columns]\n"
     ]
    }
   ],
   "source": [
    "rel_var = df.iloc[:,11:1576]\n",
    "print(rel_var.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because keras requires a numpy array as input, we convert the dataframe of our predictors to a numpy array below. To see what this looks like, uncomment the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = pd.DataFrame.as_matrix(rel_var, columns=None)\n",
    "#print(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we change our class variables to numeric values for the keras package to predict. To compare the lists, uncomment the print statement. We then use keras's to_categorical function on the numeric values to show that we're predicting multiple classes. n_cols is the number columns in the predictors dataframe, which will be fed as an argument to the model.add() function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_class = []\n",
    "\n",
    "for i in df.Class:\n",
    "    if i == 'sexism':\n",
    "        new_class.append(0)\n",
    "    elif i == 'racism':\n",
    "        new_class.append(1)\n",
    "    else:\n",
    "        new_class.append(2)\n",
    "\n",
    "#print(list(df.Class)[0:10])\n",
    "#print(new_class[0:10])\n",
    "\n",
    "target=to_categorical(new_class)\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the architecture of the model. Our model is a sequential model, and each model.add() line represents a layer in the neural network; the number after Dense( represents the number of nodes in that layer. Our activation function is \"relu\" for each layer, and \"softmax\" for the last layer (because it's predicting a categorical variable, this tells it to predict whichever class has the highest probability of being true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = Sequential()\n",
    "\n",
    "#Add layers to model\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run and compile the code. We use the adam optimizer, a loss function of categorical crossentropy (used for categorical target variables), and our metric of interest is accuracy. We also add an early stopping function with a patience of two, meaning that if our model fails to improve on our metric for two epochs, the model should stop running. The model fit function is the final piece of important code. It takes as input our predictors, our target variable, a validation split (what proportion of the dataset should be set aside for testing), and the number of epochs (this can be higher since we have an early stopping function). The accuracy of the model is the highest number under \"val_acc.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11294 samples, validate on 4841 samples\n",
      "Epoch 1/20\n",
      "11294/11294 [==============================] - 115s 10ms/step - loss: 0.7726 - acc: 0.6947 - val_loss: 0.6735 - val_acc: 0.7744\n",
      "Epoch 2/20\n",
      "11294/11294 [==============================] - 116s 10ms/step - loss: 0.6815 - acc: 0.7388 - val_loss: 0.6172 - val_acc: 0.7922\n",
      "Epoch 3/20\n",
      "11294/11294 [==============================] - 136s 12ms/step - loss: 0.6510 - acc: 0.7482 - val_loss: 0.6468 - val_acc: 0.7693\n",
      "Epoch 4/20\n",
      "11294/11294 [==============================] - 135s 12ms/step - loss: 0.6352 - acc: 0.7552 - val_loss: 0.6507 - val_acc: 0.7852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cd4a076048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(patience=2) \n",
    "\n",
    "#fit model\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=20, callbacks=[early_stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
